{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQ3JG/ioZ99xKcu7LA9jQd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Bc1SO4DmhjZO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1 is to load the text data\n",
        "with open('shakespeare.txt', 'r') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "18GBkBKOhp_D"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 2 is to create a char-to-index mapping so that the model can understand our text\n",
        "chars = sorted(list(set(text))) # unique chars in dataset\n",
        "char2idx = {char: idx for idx, char in enumerate(chars)} # map chars to indicies\n",
        "idx2char = {idx: char for idx, char in enumerate(chars)} # map indicies back to chars\n",
        "\n",
        "# convert the entire text into a sequence or integers\n",
        "data = [char2idx[char] for char in text]\n",
        "\n",
        "print(f'Number of unique chars: {len(chars)}') # how many unique chars do we have\n",
        "print(f'Example mapping: {char2idx}') # peek into the mapping"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_TsVhaeiM6u",
        "outputId": "0a3bfaed-4d1b-42d5-8485-ecc391a6cdd3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique chars: 91\n",
            "Example mapping: {'\\n': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, '&': 7, \"'\": 8, '(': 9, ')': 10, '*': 11, ',': 12, '-': 13, '.': 14, '/': 15, '0': 16, '1': 17, '2': 18, '3': 19, '4': 20, '5': 21, '6': 22, '7': 23, '8': 24, '9': 25, ':': 26, ';': 27, '<': 28, '>': 29, '?': 30, '@': 31, 'A': 32, 'B': 33, 'C': 34, 'D': 35, 'E': 36, 'F': 37, 'G': 38, 'H': 39, 'I': 40, 'J': 41, 'K': 42, 'L': 43, 'M': 44, 'N': 45, 'O': 46, 'P': 47, 'Q': 48, 'R': 49, 'S': 50, 'T': 51, 'U': 52, 'V': 53, 'W': 54, 'X': 55, 'Y': 56, 'Z': 57, '[': 58, ']': 59, '_': 60, '`': 61, 'a': 62, 'b': 63, 'c': 64, 'd': 65, 'e': 66, 'f': 67, 'g': 68, 'h': 69, 'i': 70, 'j': 71, 'k': 72, 'l': 73, 'm': 74, 'n': 75, 'o': 76, 'p': 77, 'q': 78, 'r': 79, 's': 80, 't': 81, 'u': 82, 'v': 83, 'w': 84, 'x': 85, 'y': 86, 'z': 87, '|': 88, '}': 89, '~': 90}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 3 is to define a RNN model for text generation\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "    super(RNN, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_size) # embedding layer to convert indices to vectors\n",
        "    self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True) # vanilla RNN\n",
        "    self.fc = nn.Linear(hidden_size, vocab_size) # output layer to predict the next character\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    x = self.embed(x) # convert input to embeddings\n",
        "    out, hidden = self.rnn(x, hidden) # pass through the RNN\n",
        "    out = self.fc(out) # generate predictions\n",
        "    return out, hidden\n"
      ],
      "metadata": {
        "id": "1P5IkWiBi_qD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 4 is to make a helper function to create mini-batches of the data for training\n",
        "seq_length = 100 # sequence length for training\n",
        "def get_batches(data, batch_size):\n",
        "  n_batches = len(data) // (batch_size * seq_length)\n",
        "  data = data[:n_batches * batch_size * seq_length]\n",
        "  x = np.array(data)\n",
        "  y = np.roll(x, -1) # shift targets one position, like if you have an input of present character, output is the next character\n",
        "  x = x.reshape((batch_size, -1))\n",
        "  y = y.reshape((batch_size, -1))\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "0440lF5PkyZG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64 # number of sequencies in a batch\n",
        "x, y = get_batches(data, batch_size)\n",
        "print(f'Input shape: {x.shape}, target shape: {y.shape}') # to show the data shapes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohHhXUAplnzi",
        "outputId": "5e0d54d2-bd1c-40b7-a034-dfbb629459b3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (64, 85300), target shape: (64, 85300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 5 is to train this RNN model\n",
        "def train(model, data, epochs, batch_size, seq_length, vocab_size, lr=0.001):\n",
        "  criterion = nn.CrossEntropyLoss() # loss function\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr) # optimizer\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    x, y = get_batches(data, batch_size) # get a batch of data\n",
        "    hidden = None # initialize hidden state\n",
        "    for i in range(0, x.shape[1], seq_length):\n",
        "      inputs = torch.tensor(x[:, i:i+seq_length], dtype=torch.long)\n",
        "      targets = torch.tensor(y[:, i:i+seq_length], dtype=torch.long) # the target outputs\n",
        "\n",
        "      optimizer.zero_grad() # reset gradients\n",
        "\n",
        "      # detach hidden state to prevent graph build up\n",
        "      if hidden is not None:\n",
        "        hidden = hidden.detach()\n",
        "\n",
        "      # forward pass\n",
        "      output, hidden = model(inputs, hidden)\n",
        "\n",
        "      # compute loss\n",
        "      loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
        "\n",
        "      # backprop and update\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{epochs}, loss: {loss.item():.5f}') # progress update\n",
        "\n"
      ],
      "metadata": {
        "id": "N0dBhj4Bl84X"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters for RNN\n",
        "vocab_size = len(chars)\n",
        "embed_size = 128\n",
        "hidden_size = 256\n",
        "num_layers = 2"
      ],
      "metadata": {
        "id": "mZRjnpm3nbDf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize and train the model\n",
        "model = RNN(vocab_size, embed_size, hidden_size, num_layers)\n",
        "train(model,data, epochs=10, batch_size=64, seq_length=100, vocab_size=vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0hWrPsNnjU3",
        "outputId": "65e81042-ecad-4212-980d-f42b35bb323d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, loss: 1.44987\n",
            "Epoch 2/10, loss: 1.35407\n",
            "Epoch 3/10, loss: 1.31232\n",
            "Epoch 4/10, loss: 1.28601\n",
            "Epoch 5/10, loss: 1.26744\n",
            "Epoch 6/10, loss: 1.25524\n",
            "Epoch 7/10, loss: 1.24523\n",
            "Epoch 8/10, loss: 1.23815\n",
            "Epoch 9/10, loss: 1.23252\n",
            "Epoch 10/10, loss: 1.22719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # next generate text by the model\n",
        "def generate_text(model, start_char, length, hidden):\n",
        "  model.eval() # switch to evaluation mode\n",
        "  input = torch.tensor([[char2idx[start_char]]], dtype=torch.long)\n",
        "  generated = start_char # start the text with the start character\n",
        "\n",
        "  for _ in range(length):\n",
        "    output, hidden = model(input, hidden)\n",
        "    prob = nn.functional.softmax(output.squeeze(), dim=-1).data # predict probabilities\n",
        "    char_idx = torch.multinomial(prob, 1).item() # sample from probabilities\n",
        "    generated += idx2char[char_idx] # append the character to the output\n",
        "    input = torch.tensor([[char_idx]], dtype=torch.long) # update the input\n",
        "\n",
        "  return generated"
      ],
      "metadata": {
        "id": "9bKD76mbu8Tw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 1000 characters of text starting with 'B'\n",
        "hidden = None # reset hidden state\n",
        "print(generate_text(model, start_char='B', length=1000, hidden=hidden))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuMN7L5CyrFT",
        "outputId": "e966d672-596f-4d8d-b801-17f3243eb7a3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOLAMANA, and HERMIONE and unthinks again,\n",
            "    Speak it.\n",
            "  BEROWNE. No, if I warrant you; an honest Suffolk; well!\n",
            "  CONSTABLE. And so Gallingrable-on the moon-caked her monument,\n",
            "    Speak \"\n",
            "  WARG HAMWEND MACHINE RICHARD, SALESS thereon, my does\n",
            "    By hole wife, that no more to leave, and all the high modor? I'll use your degned devil!\n",
            "  CAIUS. Stay, which you made but carried to Octiodwallow to th' ear so import whilst I know\n",
            "    Had\n",
            "    then; holly some cheticles trunk to the midderly in\n",
            "    the bow of wine,\n",
            "  And, in the Christian\n",
            "    provese be advis'd sort, LIRION youtless the world from wedax'd\n",
            "    The gross of worthless; so\n",
            "    Could make it, look doth be here.\n",
            "\n",
            "  I\"-\n",
            "  MESSENGER. Whitwer, thou can wrongs and milthal eye.\n",
            "  HEBREY. What spoke this and false certain another!\n",
            "  Fal. Display perfect:\n",
            "    For that if I love him, you was,\n",
            "    The gods, I faccutaties\n",
            "    Romeligious and Lover, sir?\n",
            "  SHELONS\n",
            "\n",
            "  CLOWN. Gentle, Northumble,\n",
            "    With the starbing of love, and most the h\n"
          ]
        }
      ]
    }
  ]
}